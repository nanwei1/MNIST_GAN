{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 32\n",
    "hidden1_size = 64\n",
    "hidden2_size = 128\n",
    "image_size = 784\n",
    "num_epochs = 30\n",
    "batch_size = 32\n",
    "sample_dir = 'samples'\n",
    "save_dir = 'save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory if not exists\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image processing\n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor()])\n",
    "\n",
    "# MNIST dataset\n",
    "mnist = torchvision.datasets.MNIST(root='./data/',\n",
    "                                   train=True,\n",
    "                                   transform=transform,\n",
    "                                   download=True)\n",
    "\n",
    "# Data loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(1.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1b61574eb8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgs, lbls = next(iter(data_loader))\n",
    "imgs[0].data.shape\n",
    "print(imgs.data.min())\n",
    "print(imgs.data.max())\n",
    "plt.imshow(imgs[0].data.reshape((28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "D = nn.Sequential(\n",
    "    nn.Linear(image_size, hidden2_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden2_size, hidden1_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden1_size, 1),\n",
    "    nn.Sigmoid())\n",
    "\n",
    "# Generator \n",
    "G = nn.Sequential(\n",
    "    nn.Linear(latent_size, hidden1_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden1_size, hidden2_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden2_size, image_size),\n",
    "    nn.Sigmoid())\n",
    "\n",
    "\n",
    "# Binary cross entropy loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=5e-4)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_grad():\n",
    "    d_optimizer.zero_grad()\n",
    "    g_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30] took 19.731241464614868 seconds\n",
      "Epoch [2/30] took 25.713005542755127 seconds\n",
      "Epoch [3/30] took 21.787495851516724 seconds\n",
      "Epoch [4/30] took 22.809000730514526 seconds\n",
      "Epoch [5/30] took 24.66292929649353 seconds\n",
      "Epoch [6/30] took 24.88949751853943 seconds\n",
      "Epoch [7/30] took 20.7968692779541 seconds\n",
      "Epoch [8/30] took 19.66516137123108 seconds\n",
      "Epoch [9/30] took 19.494091033935547 seconds\n",
      "Epoch [10/30] took 19.90772819519043 seconds\n",
      "Epoch [11/30] took 25.21996569633484 seconds\n",
      "Epoch [12/30] took 26.50868535041809 seconds\n",
      "Epoch [13/30] took 29.29830527305603 seconds\n",
      "Epoch [14/30] took 29.206934213638306 seconds\n",
      "Epoch [15/30] took 25.179628610610962 seconds\n",
      "Epoch [16/30] took 23.997515201568604 seconds\n",
      "Epoch [17/30] took 26.714616060256958 seconds\n",
      "Epoch [18/30] took 27.44491195678711 seconds\n",
      "Epoch [19/30] took 24.039395093917847 seconds\n",
      "Epoch [20/30] took 24.355393648147583 seconds\n",
      "Epoch [21/30] took 30.30539059638977 seconds\n",
      "Epoch [22/30] took 30.071666479110718 seconds\n",
      "Epoch [23/30] took 25.137555599212646 seconds\n",
      "Epoch [24/30] took 30.28367853164673 seconds\n",
      "Epoch [25/30] took 29.80913233757019 seconds\n",
      "Epoch [26/30] took 29.72523546218872 seconds\n",
      "Epoch [27/30] took 30.36133599281311 seconds\n",
      "Epoch [28/30] took 30.16550302505493 seconds\n",
      "Epoch [29/30] took 29.33010196685791 seconds\n",
      "Epoch [30/30] took 31.13629674911499 seconds\n"
     ]
    }
   ],
   "source": [
    "# Statistics to be saved\n",
    "d_losses = np.zeros(num_epochs)\n",
    "g_losses = np.zeros(num_epochs)\n",
    "real_scores = np.zeros(num_epochs)\n",
    "fake_scores = np.zeros(num_epochs)\n",
    "\n",
    "# Start training\n",
    "start_time = time.time()\n",
    "total_step = len(data_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    for i, (images, _) in enumerate(data_loader):\n",
    "        images = images.view(images.size(0), -1)\n",
    "#         images = Variable(images)\n",
    "        # Create the labels which are later used as input for the BCE loss\n",
    "        real_labels = torch.ones(images.size(0), 1)\n",
    "#         real_labels = Variable(real_labels)\n",
    "        fake_labels = torch.zeros(images.size(0), 1)\n",
    "#         fake_labels = Variable(fake_labels)\n",
    "\n",
    "        # ================================================================== #\n",
    "        #                      Train the discriminator                       #\n",
    "        # ================================================================== #\n",
    "\n",
    "        # Compute BCE_Loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))\n",
    "        # Second term of the loss is always zero since real_labels == 1\n",
    "        outputs = D(images)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "        \n",
    "        # Compute BCELoss using fake images\n",
    "        # First term of the loss is always zero since fake_labels == 0\n",
    "        reset_grad()\n",
    "        z = torch.randn(images.size(0), latent_size)\n",
    "#         z = Variable(z)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        # If D is trained so well, then don't update\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        # ================================================================== #\n",
    "        #                        Train the generator                         #\n",
    "        # ================================================================== #\n",
    "\n",
    "        # Compute loss with fake images\n",
    "        reset_grad()\n",
    "        z = torch.randn(images.size(0), latent_size)\n",
    "#         z = Variable(z)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        \n",
    "        # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))\n",
    "        # For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        # if G is trained so well, then don't update\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        # =================================================================== #\n",
    "        #                          Update Statistics                          #\n",
    "        # =================================================================== #\n",
    "        d_losses[epoch] = d_losses[epoch]*(i/(i+1.)) + d_loss.item()*(1./(i+1.))\n",
    "        g_losses[epoch] = g_losses[epoch]*(i/(i+1.)) + g_loss.item()*(1./(i+1.))\n",
    "        real_scores[epoch] = real_scores[epoch]*(i/(i+1.)) + real_score.mean().item()*(1./(i+1.))\n",
    "        fake_scores[epoch] = fake_scores[epoch]*(i/(i+1.)) + fake_score.mean().item()*(1./(i+1.))\n",
    "        \n",
    "    print('Epoch [{}/{}] took {} seconds'.format(epoch+1, num_epochs, time.time()-epoch_start_time))\n",
    "    # Save real images\n",
    "    if (epoch+1) == 1:\n",
    "        images = images.view(images.size(0), 1, 28, 28)\n",
    "        save_image(images.data, os.path.join(sample_dir, 'real_images.png'))\n",
    "    \n",
    "    # Save sampled images\n",
    "    fake_images = fake_images.view(fake_images.size(0), 1, 28, 28)\n",
    "    save_image(fake_images.data, os.path.join(sample_dir, 'fake_images-{}.png'.format(epoch+1)))\n",
    "    \n",
    "    # Save and plot Statistics\n",
    "    np.save(os.path.join(save_dir, 'd_losses.npy'), d_losses)\n",
    "    np.save(os.path.join(save_dir, 'g_losses.npy'), g_losses)\n",
    "    np.save(os.path.join(save_dir, 'fake_scores.npy'), fake_scores)\n",
    "    np.save(os.path.join(save_dir, 'real_scores.npy'), real_scores)\n",
    "    \n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim([0, num_epochs + 1])\n",
    "    plt.plot(range(1, num_epochs + 1), d_losses, label='d loss')\n",
    "    plt.plot(range(1, num_epochs + 1), g_losses, label='g loss')    \n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, 'loss.pdf'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim([0, num_epochs + 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    plt.plot(range(1, num_epochs + 1), fake_scores, label='fake score')\n",
    "    plt.plot(range(1, num_epochs + 1), real_scores, label='real score')    \n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, 'accuracy.pdf'))\n",
    "    plt.close()\n",
    "\n",
    "    # Save model at checkpoints\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        torch.save(G.state_dict(), os.path.join(save_dir, 'G--{}.ckpt'.format(epoch+1)))\n",
    "        torch.save(D.state_dict(), os.path.join(save_dir, 'D--{}.ckpt'.format(epoch+1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(save_dir, 'd_losses.npy'), d_losses)\n",
    "np.save(os.path.join(save_dir, 'g_losses.npy'), g_losses)\n",
    "np.save(os.path.join(save_dir, 'fake_scores.npy'), fake_scores)\n",
    "np.save(os.path.join(save_dir, 'real_scores.npy'), real_scores)\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([0, num_epochs + 1])\n",
    "plt.plot(range(1, num_epochs + 1), d_losses, label='d loss')\n",
    "plt.plot(range(1, num_epochs + 1), g_losses, label='g loss')    \n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(save_dir, 'loss.pdf'))\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([0, num_epochs + 1])\n",
    "ax.set_ylim([0, 1])\n",
    "plt.plot(range(1, num_epochs + 1), fake_scores, label='fake score')\n",
    "plt.plot(range(1, num_epochs + 1), real_scores, label='real score')    \n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(save_dir, 'accuracy.pdf'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
