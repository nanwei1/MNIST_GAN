{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 128\n",
    "image_size = 784\n",
    "init_size = 28/4 #initial size of generated image before upsampling\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "sample_dir = 'samples'\n",
    "save_dir = 'save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory if not exists\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image processing\n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor()])\n",
    "\n",
    "# MNIST dataset\n",
    "mnist = torchvision.datasets.MNIST(root='../data/',\n",
    "                                   train=True,\n",
    "                                   transform=transform)\n",
    "\n",
    "# Data loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(1.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7feff3295f28>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgs, lbls = next(iter(data_loader))\n",
    "imgs[0].data.shape\n",
    "print(imgs.data.min())\n",
    "print(imgs.data.max())\n",
    "plt.imshow(imgs[0].data.reshape((28,28)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=4,kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1),\n",
    "            nn.Conv2d(in_channels=4,out_channels=8,kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(8*(28-2*2-2*2)**2,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_size, int(8*init_size**2)))\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(8, 4, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(4, 1, 3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = out.view(out.shape[0], 8, int(init_size), int(init_size))\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "    \n",
    "D = Discriminator()\n",
    "G = Generator()\n",
    "# Binary cross entropy loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=5e-4)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_grad():\n",
    "    d_optimizer.zero_grad()\n",
    "    g_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics to be saved\n",
    "d_losses = np.zeros(num_epochs)\n",
    "g_losses = np.zeros(num_epochs)\n",
    "real_scores = np.zeros(num_epochs)\n",
    "fake_scores = np.zeros(num_epochs)\n",
    "\n",
    "# Start training\n",
    "start_time = time.time()\n",
    "total_step = len(data_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    for i, (images, _) in enumerate(data_loader):\n",
    "        # Create the labels which are later used as input for the BCE loss\n",
    "        real_labels = torch.ones(images.size(0), 1)\n",
    "        fake_labels = torch.zeros(images.size(0), 1)\n",
    "\n",
    "        # ================================================================== #\n",
    "        #                      Train the discriminator                       #\n",
    "        # ================================================================== #\n",
    "\n",
    "        # Compute BCE_Loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))\n",
    "        # Second term of the loss is always zero since real_labels == 1\n",
    "        outputs = D(images)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "        \n",
    "        # Compute BCELoss using fake images\n",
    "        # First term of the loss is always zero since fake_labels == 0\n",
    "        reset_grad()\n",
    "        z = torch.randn(images.size(0), latent_size)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        # If D is trained so well, then don't update\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        # ================================================================== #\n",
    "        #                        Train the generator                         #\n",
    "        # ================================================================== #\n",
    "\n",
    "        # Compute loss with fake images\n",
    "        reset_grad()\n",
    "        z = torch.randn(batch_size, latent_size)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        \n",
    "        # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))\n",
    "        # For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf\n",
    "        real_labels = torch.ones(fake_images.size(0), 1)\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        # =================================================================== #\n",
    "        #                          Update Statistics                          #\n",
    "        # =================================================================== #\n",
    "        d_losses[epoch] = d_losses[epoch]*(i/(i+1.)) + d_loss.item()*(1./(i+1.))\n",
    "        g_losses[epoch] = g_losses[epoch]*(i/(i+1.)) + g_loss.item()*(1./(i+1.))\n",
    "        real_scores[epoch] = real_scores[epoch]*(i/(i+1.)) + real_score.mean().item()*(1./(i+1.))\n",
    "        fake_scores[epoch] = fake_scores[epoch]*(i/(i+1.)) + fake_score.mean().item()*(1./(i+1.))\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print('Epoch %d / %d took %6.2f seconds' % (epoch+1, num_epochs, time.time()-epoch_start_time))\n",
    "    print('Total training time till this epoch was %8.2f seconds' % (time.time()-start_time))\n",
    "    # Save real images\n",
    "    if (epoch+1) == 1:\n",
    "        images = images.view(images.size(0), 1, 28, 28)\n",
    "        save_image(1-images.data, os.path.join(sample_dir, 'real_images.png'))\n",
    "    \n",
    "    # Save sampled images\n",
    "    fake_images = fake_images.view(fake_images.size(0), 1, 28, 28)\n",
    "    save_image(1-fake_images.data, os.path.join(sample_dir, 'fake_images-{}.png'.format(epoch+1)))\n",
    "    \n",
    "    plt.figure(figsize=(12,8))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim([0, num_epochs + 1])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(range(1, epoch + 2), d_losses[:epoch+1], label='Discriminator Loss')\n",
    "    plt.plot(range(1, epoch + 2), g_losses[:epoch+1], label='Generator Loss')    \n",
    "    plt.legend()\n",
    "    ax.grid(linestyle='-.')\n",
    "    plt.show()\n",
    "\n",
    "    # Save model at checkpoints\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        torch.save(G.state_dict(), os.path.join(save_dir, 'G--{}.ckpt'.format(epoch+1)))\n",
    "        torch.save(D.state_dict(), os.path.join(save_dir, 'D--{}.ckpt'.format(epoch+1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(save_dir, 'd_losses.npy'), d_losses)\n",
    "np.save(os.path.join(save_dir, 'g_losses.npy'), g_losses)\n",
    "np.save(os.path.join(save_dir, 'fake_scores.npy'), fake_scores)\n",
    "np.save(os.path.join(save_dir, 'real_scores.npy'), real_scores)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([0, num_epochs + 1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(range(1, num_epochs + 1), d_losses, label='d loss')\n",
    "plt.plot(range(1, num_epochs + 1), g_losses, label='g loss')    \n",
    "plt.legend()\n",
    "ax.grid(linestyle='-.')\n",
    "plt.savefig(os.path.join(save_dir, 'loss.png'))\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([0, num_epochs + 1])\n",
    "ax.set_ylim([0, 1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.plot(range(1, num_epochs + 1), fake_scores, label='fake score')\n",
    "plt.plot(range(1, num_epochs + 1), real_scores, label='real score')    \n",
    "plt.legend()\n",
    "ax.grid(linestyle='-.')\n",
    "plt.savefig(os.path.join(save_dir, 'accuracy.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
