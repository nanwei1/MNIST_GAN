{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 128\n",
    "image_size = 784\n",
    "init_size = 28/4 #initial size of generated image before upsampling\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "sample_dir = 'samples'\n",
    "save_dir = 'save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory if not exists\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image processing\n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor()])\n",
    "\n",
    "# MNIST dataset\n",
    "mnist = torchvision.datasets.MNIST(root='../data/',\n",
    "                                   train=True,\n",
    "                                   transform=transform)\n",
    "\n",
    "# Data loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(1.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9653825f60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgs, lbls = next(iter(data_loader))\n",
    "imgs[0].data.shape\n",
    "print(imgs.data.min())\n",
    "print(imgs.data.max())\n",
    "plt.imshow(imgs[0].data.reshape((28,28)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=4,kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1),\n",
    "            nn.Conv2d(in_channels=4,out_channels=8,kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(8*(28-2*2-2*2)**2,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_size, int(8*init_size**2)))\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(8, 4, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(4, 1, 3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = out.view(out.shape[0], 8, int(init_size), int(init_size))\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "    \n",
    "D = Discriminator()\n",
    "G = Generator()\n",
    "# Binary cross entropy loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=5e-4)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_grad():\n",
    "    d_optimizer.zero_grad()\n",
    "    g_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f22bc6d33de6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# If D is trained so well, then don't update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_loss_real\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0md_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# ================================================================== #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep-learning/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep-learning/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Statistics to be saved\n",
    "d_losses = np.zeros(num_epochs)\n",
    "g_losses = np.zeros(num_epochs)\n",
    "real_scores = np.zeros(num_epochs)\n",
    "fake_scores = np.zeros(num_epochs)\n",
    "\n",
    "# Start training\n",
    "start_time = time.time()\n",
    "total_step = len(data_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    for i, (images, _) in enumerate(data_loader):\n",
    "        # Create the labels which are later used as input for the BCE loss\n",
    "        real_labels = torch.ones(images.size(0), 1)\n",
    "        fake_labels = torch.zeros(images.size(0), 1)\n",
    "\n",
    "        # ================================================================== #\n",
    "        #                      Train the discriminator                       #\n",
    "        # ================================================================== #\n",
    "\n",
    "        # Compute BCE_Loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))\n",
    "        # Second term of the loss is always zero since real_labels == 1\n",
    "        outputs = D(images)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "        \n",
    "        # Compute BCELoss using fake images\n",
    "        # First term of the loss is always zero since fake_labels == 0\n",
    "        reset_grad()\n",
    "        z = torch.randn(images.size(0), latent_size)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        # If D is trained so well, then don't update\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        # ================================================================== #\n",
    "        #                        Train the generator                         #\n",
    "        # ================================================================== #\n",
    "\n",
    "        # Compute loss with fake images\n",
    "        reset_grad()\n",
    "        z = torch.randn(batch_size, latent_size)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        \n",
    "        # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))\n",
    "        # For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf\n",
    "        real_labels = torch.ones(fake_images.size(0), 1)\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        # =================================================================== #\n",
    "        #                          Update Statistics                          #\n",
    "        # =================================================================== #\n",
    "        d_losses[epoch] = d_losses[epoch]*(i/(i+1.)) + d_loss.item()*(1./(i+1.))\n",
    "        g_losses[epoch] = g_losses[epoch]*(i/(i+1.)) + g_loss.item()*(1./(i+1.))\n",
    "        real_scores[epoch] = real_scores[epoch]*(i/(i+1.)) + real_score.mean().item()*(1./(i+1.))\n",
    "        fake_scores[epoch] = fake_scores[epoch]*(i/(i+1.)) + fake_score.mean().item()*(1./(i+1.))\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print('Epoch %d / %d took %6.2f seconds' % (epoch+1, num_epochs, time.time()-epoch_start_time))\n",
    "    print('Total training time till this epoch was %8.2f seconds' % (time.time()-start_time))\n",
    "    # Save real images\n",
    "    if (epoch+1) == 1:\n",
    "        images = images.view(images.size(0), 1, 28, 28)\n",
    "        save_image(1-images.data, os.path.join(sample_dir, 'real_images.png'))\n",
    "    \n",
    "    # Save sampled images\n",
    "    fake_images = fake_images.view(fake_images.size(0), 1, 28, 28)\n",
    "    save_image(1-fake_images.data, os.path.join(sample_dir, 'fake_images-{}.png'.format(epoch+1)))\n",
    "    \n",
    "    plt.figure(figsize=(12,8))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim([0, num_epochs + 1])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(range(1, epoch + 2), d_losses[:epoch+1], label='Discriminator Loss')\n",
    "    plt.plot(range(1, epoch + 2), g_losses[:epoch+1], label='Generator Loss')    \n",
    "    plt.legend()\n",
    "    ax.grid(linestyle='-.')\n",
    "    plt.show()\n",
    "\n",
    "    # Save model at checkpoints\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        torch.save(G.state_dict(), os.path.join(save_dir, 'G--{}.ckpt'.format(epoch+1)))\n",
    "        torch.save(D.state_dict(), os.path.join(save_dir, 'D--{}.ckpt'.format(epoch+1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(save_dir, 'd_losses.npy'), d_losses)\n",
    "np.save(os.path.join(save_dir, 'g_losses.npy'), g_losses)\n",
    "np.save(os.path.join(save_dir, 'fake_scores.npy'), fake_scores)\n",
    "np.save(os.path.join(save_dir, 'real_scores.npy'), real_scores)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([0, num_epochs + 1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(range(1, num_epochs + 1), d_losses, label='d loss')\n",
    "plt.plot(range(1, num_epochs + 1), g_losses, label='g loss')    \n",
    "plt.legend()\n",
    "ax.grid(linestyle='-.')\n",
    "plt.savefig(os.path.join(save_dir, 'loss.png'))\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([0, num_epochs + 1])\n",
    "ax.set_ylim([0, 1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.plot(range(1, num_epochs + 1), fake_scores, label='fake score')\n",
    "plt.plot(range(1, num_epochs + 1), real_scores, label='real score')    \n",
    "plt.legend()\n",
    "ax.grid(linestyle='-.')\n",
    "plt.savefig(os.path.join(save_dir, 'accuracy.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
